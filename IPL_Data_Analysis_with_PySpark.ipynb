{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneetpushkar/IPL-Data-Analysis-with-PySpark/blob/main/IPL_Data_Analysis_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 01: Environment Setup"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b5610ec4-4c84-4462-9f75-0d81a39cefb1"
        },
        "id": "PuL4CKPYtkP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "S3j1SSMxv5m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.add_packages('mysql:mysql-connector-java:8.0.11')"
      ],
      "metadata": {
        "id": "uJlcvq7Evd95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "rfjkpTWUwO7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "7Flmopi0wnYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_by_ball_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/ipl_ball_by_ball.csv\")\n",
        "\n",
        "ipl_venue_df= spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/ipl_venue.csv\")\n",
        "\n",
        "ipl_matches_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"ipl_matches.csv\")"
      ],
      "metadata": {
        "id": "L7oWqPnMN6qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ball_by_ball_df.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27b46638-2ede-4f91-8c98-708a6413ef71"
        },
        "id": "QshPKHn4tkQP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ipl_venue_df.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "36eb80d2-8464-4ba7-a765-a7d7e458d261"
        },
        "id": "JdbCMy6JtkQQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ipl_matches_df.printSchema()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "36a79e65-02f3-4996-8cd3-e3e5744afcaf"
        },
        "id": "FIimMArQtkQQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating table from dataframe\n",
        "\n",
        "ipl_venue_df.createOrReplaceTempView(\"ipl_venue\")\n",
        "spark.sql(\"select * from ipl_venue\")\n",
        "ipl_matches_df.createOrReplaceTempView(\"ipl_matches\")\n",
        "spark.sql(\"select * from ipl_matches\")\n",
        "ball_by_ball_df.createOrReplaceTempView(\"ipl_ball_by_ball\")\n",
        "spark.sql(\"select * from ipl_ball_by_ball\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2f0e5ba0-a45b-4fed-b02f-45f1b9bd98de"
        },
        "id": "OItVbu1wtkQS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 02: Analysis"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3f936a48-46f3-4fe2-8e50-e76e190c82f4"
        },
        "id": "C9lXtGottkQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question A:\n",
        "Find the top 3 venues which hosted the most number of eliminator\n",
        "matches?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9604091a-5079-4189-a204-cbb3ff29af3b"
        },
        "id": "uG7_KkkgtkQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select v.venue, count(v.venue_id) venue_count from ipl_matches m \n",
        "join ipl_venue v on m.venue_id=v.venue_id\n",
        "where m.eliminator = 'Y'\n",
        "group by v.venue\n",
        "order by venue_count desc\n",
        "limit 3;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7bdc79f4-1777-4787-bede-2338c5ba438f"
        },
        "id": "ZlyHO5VftkQX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question B: \n",
        "Return most number of catches taken by a player in IPL history?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a04227ec-0905-4994-a26c-ababef67d3a0"
        },
        "id": "NJCKryQotkQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select fielder, count(dismissal_kind) NoOfCatches from ipl_ball_by_ball\n",
        "where dismissal_kind = \"caught\"\n",
        "group by fielder\n",
        "order by NoOfCatches desc\n",
        "limit 1;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1a21fe2e-4b76-4ccb-98bc-2b734903a741"
        },
        "id": "_mOcbuJ9tkQe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question C:\n",
        "Write a query to return a report for highest wicket taker in matches\n",
        "which were affected by Duckworth-Lewisâ€™s method (D/L method)."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c2971229-4260-4dbe-9970-fbb8eba7cee4"
        },
        "id": "oBmHDfEotkQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select m.match_id, bbb.bowler, count(bbb.bowler) NoOfWickets from ipl_ball_by_ball bbb\n",
        "join ipl_matches m on m.match_id = bbb.match_id\n",
        "where m.method = \"D/L\" and is_wicket = 1\n",
        "group by m.match_id, bbb.bowler\n",
        "order by NoOfWickets desc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d8202f88-71b5-4767-a1fb-b7e51222cf5b"
        },
        "id": "KW3j3IWKtkQl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question D: \n",
        "Write a query to return a report for highest strike rate by a batsman in non\n",
        "powerplay overs(7-20 overs)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a8198ac4-d660-416d-b04c-1786ddfee495"
        },
        "id": "WgAObSp-tkQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select batsman, (sum(batsman_runs)/count(ball)*100) as strikeRate from ipl_ball_by_ball\n",
        "where (extras_type not in (\"wides\",\"noballs\")) \n",
        "and (overs >= 7 and overs <= 20)\n",
        "group by batsman\n",
        "order by strikeRate desc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "214fcb66-e29f-4677-802e-a44654103686"
        },
        "id": "6A5arK9HtkQn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question E.\n",
        "Write a query to return a report for highest extra runs in a venue (stadium, city)."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "009e5126-07d7-48f7-8206-68507fc312ff"
        },
        "id": "5rEwBDnCtkQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select v.venue, v.city, sum(bbb.extra_runs) extra_runs from ipl_ball_by_ball bbb\n",
        "inner join ipl_matches as m on bbb.match_id=m.match_id\n",
        "inner join ipl_venue as v on m.venue_id =v.venue_id\n",
        "group by venue, v.city\n",
        "order by extra_runs desc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da679bab-37df-4cb2-969b-4a81c68fed81"
        },
        "id": "_Uh3nO2XtkQp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question F: \n",
        "Write a query to return a report for the cricketers with the most number of players of\n",
        "the match award in neutral venues"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "04dd0875-56d8-4089-a6f8-929ac6587552"
        },
        "id": "zN4HCSXdtkQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select m.player_of_match, count(m.player_of_match) Count_Of_Player_Of_Match from ipl_matches m\n",
        "join ipl_venue v on m.venue_id = v.venue_id\n",
        "where neutral_venue = 1\n",
        "group by m.player_of_match\n",
        "order by count(m.player_of_match) desc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f5620c85-b3d5-4af2-9aa0-d348c383c190"
        },
        "id": "cCQCC57JtkQ2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question G: \n",
        "Write a query to get a list of top 10 players with the highest batting average"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6d709ac0-8aee-4d3f-afa2-69bb1fa7b743"
        },
        "id": "Bp1Jv--xtkQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select Batsman_, sum(batsman_runs)/count(player_dismissed) as Average \n",
        "from\n",
        "(\n",
        " (select batsman as Batsman_,batsman_runs,player_dismissed from IPL_BALL_BY_BALL where player_dismissed != 'NA')\n",
        ")\n",
        "group by Batsman_ \n",
        "order by Average desc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "091a6df0-8db9-4554-9865-ccbaa4c8c41c"
        },
        "id": "f7A3V7qbtkQ3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question H:\n",
        "Write a query to find out who has officiated (as an umpire) the most\n",
        "number of matches in IPL"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7968975f-8bf1-46ff-aae9-1f7ecda6007d"
        },
        "id": "sHdpIMf7tkQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select umpire, count(*) as No_of_Matches\n",
        "from ((select umpire1 as umpire from ipl_matches) union all\n",
        "      (select umpire2 from ipl_matches)\n",
        "     ) ipl_matches\n",
        "group by umpire\n",
        "order by count(*) desc ;\n",
        "\"\"\").show()\n",
        "#Combing two similar columns by union and counting numbers on times they have officiated"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "995bf660-5405-43c6-a5b6-25a59034da36"
        },
        "id": "PWhqml1stkQ6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question I:  \n",
        "Find venue details of the match where V Kohli scored his highest individual runs in\n",
        "IPL."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "437cd7ed-f7fe-4971-a191-f93a1b85bff9"
        },
        "id": "sUyieucctkQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\n",
        "\"\"\"\n",
        "select m.match_id, v.venue, v.city, sum(b.batsman_runs) TotalRuns from ipl_ball_by_ball as b\n",
        "inner join ipl_matches as m on b.match_id=m.match_id\n",
        "inner join ipl_venue as v on m.venue_id =v.venue_id\n",
        "where b.batsman= 'V Kohli'\n",
        "group by m.match_id, v.venue, v.city\n",
        "order by sum(b.batsman_runs) desc limit 1; \n",
        "\"\"\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f6b712e-5b48-49e1-b342-bc9a2f1c8673"
        },
        "id": "iSKOF3YetkQ-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question J: \n",
        "Please analyze how winning/losing tosses can impact a match and it's result?\n",
        "(Bonus for Visualization here)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7dd177f0-1af2-4484-a4b1-647901a5e8fc"
        },
        "id": "JEYjaMVYtkRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# count data where match_winner = toss_winner\n",
        "team_wins_df = ipl_matches_df.filter(F.col('winner')==F.col('toss_winner')).groupBy('winner').agg(F.count('winner').alias('team_wins'))\n",
        "\n",
        "# count no of toss winner\n",
        "toss_wins_df = ipl_matches_df.groupBy('toss_winner').count().orderBy('toss_winner')\n",
        "#print(team_wins_df.show(2))\n",
        "#print(toss_wins_df.show(2))\n",
        "\n",
        "toss_wins_df = toss_wins_df.withColumnRenamed(\"toss_winner\", \"winner\").withColumnRenamed(\"count\", \"toss_wins\")\n",
        "\n",
        "# join the two dataframes (team_wins_df & toss_wins_df)\n",
        "team_toss_wins = team_wins_df.join(toss_wins_df.select('winner', 'toss_wins'), ['winner'])\n",
        "team_toss_wins = team_toss_wins.withColumnRenamed(\"winner\", \"teams\")\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ee79e6ce-2223-4ef3-b715-3cb39a844d1e"
        },
        "id": "Kd86JnXstkRA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# converting spark dataframes to pandas dataframe\n",
        "\n",
        "team_toss_wins = team_toss_wins.toPandas()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "383ce79b-b038-4ebc-8486-2f660b3e429e"
        },
        "id": "fxmdDaX8tkRC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph with respect to match wins vs toss wins\n",
        "\n",
        "team_toss_wins = team_toss_wins.sort_values(by=['team_wins', 'toss_wins'], ascending = False)\n",
        "team_toss_wins.plot(x=\"teams\", y=[\"team_wins\", \"toss_wins\"], kind=\"bar\", title = \"Team & Toss Wins\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4a015bdf-ff16-4dbf-89f5-a53760f2e0ef"
        },
        "id": "54ONkzNetkRD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "win_count = 0\n",
        "for value in ipl_matches_df.collect():\n",
        "    if(value['toss_winner']==value['winner']):\n",
        "        win_count += 1\n",
        "print(f'The number of times the team winning toss have won: {win_count}')\n",
        "prob = win_count/(ipl_matches_df.count())\n",
        "print('The probability of winning if won the toss: {:.2f}' .format(prob))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0ef26d18-b399-47ab-af33-42c6b595adb6"
        },
        "id": "N-OeQB_4tkRE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the analysis we can say that :<br>\n",
        "1. The team with highest no. of toss wins has mostly won the match most of the times.<br>\n",
        "2. A team will have ***51%*** chance of winnning the match if it wins the toss"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0fe2d18f-ad18-4aaa-923d-a29e71ed1115"
        },
        "id": "44Nxh9KatkRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 03: Expose Data"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "48b4b4ca-d0c6-4844-b7a6-d1985364feb6"
        },
        "id": "S5Hk2M5WtkRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# creating sqlite3 database\n",
        "con = sqlite3.connect('iplmatches.db')"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1a83bb9d-bb14-47c9-b686-ed118cdcc0f9"
        },
        "id": "S5kU9bFttkRG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "\n",
        "df = pd.read_csv('/content/ipl_ball_by_ball.csv')\n",
        "\n",
        "# strip whitespace from headers\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# drop data into database\n",
        "df.to_sql(\"ipl_ball_by_ball\", con)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c3542d3f-224e-4cb0-abca-c5fd71771171"
        },
        "id": "YrAqZr6_tkRH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv('/content/ipl_venue.csv')\n",
        "\n",
        "# strip whitespace from headers\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# drop data into database\n",
        "df.to_sql(\"ipl_venue\", con)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ad18dbf7-7dd2-4925-98da-f501583cd929"
        },
        "id": "GGILlJg8tkRI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv('/content/ipl_matches.csv')\n",
        "\n",
        "# strip whitespace from headers\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# drop data into database\n",
        "df.to_sql(\"ipl_matches\", con)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bf2b7989-fcd2-48d1-a73c-8ae774234633"
        },
        "id": "TO-oKkd9tkRJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Database:\n",
        "    # constructor \n",
        "    def __init__(self):\n",
        "        self.db_name = \"iplmatches.db\"\n",
        "        \n",
        "    def get_conn(self):\n",
        "        try:\n",
        "            con = sqlite3.connect(self.db_name)\n",
        "            print(\"Database connected successfully\")\n",
        "            return con\n",
        "        except:\n",
        "            print(\"Unable to connect data base\")\n",
        "            \n",
        "    def get_status(self,con):\n",
        "        try:\n",
        "            con.cursor()\n",
        "            print(\"Database is connected\")\n",
        "        except Exception as ex:\n",
        "            print(\"Database is not connected\")\n",
        "            \n",
        "    def close_conn(self,con):\n",
        "        try:\n",
        "            con.close()\n",
        "            print(\"Database connection closed successfully\")\n",
        "        except:\n",
        "            print(\"Unable to close Database connection\")\n",
        "            \n",
        "    # Question(A)\n",
        "    def get_query1_result(self,con):\n",
        "        cur = con.execute(\"\"\"select v.venue, count(v.venue_id) venue_count from ipl_matches m \n",
        "                         join ipl_venue v on m.venue_id=v.venue_id\n",
        "                        where m.eliminator = 'Y'\n",
        "                        group by v.venue\n",
        "                        order by venue_count desc\n",
        "                            limit 3;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "      \n",
        "    # Question (B)\n",
        "    def get_query2_result(self, con):\n",
        "        cur = con.execute(\"\"\"select fielder, count(dismissal_kind) NoOfCatches from ipl_ball_by_ball\n",
        "                           where dismissal_kind = \"caught\"\n",
        "                           group by fielder\n",
        "                           order by NoOfCatches desc\n",
        "                           limit 1;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (C)\n",
        "    def get_query3_result(self, con):\n",
        "        cur=con.execute(\"\"\"select m.match_id, bbb.bowler, count(bbb.bowler) NoOfWickets from ipl_ball_by_ball bbb\n",
        "                           join ipl_matches m on m.match_id = bbb.match_id\n",
        "                           where m.method = \"D/L\" and is_wicket = 1\n",
        "                           group by m.match_id, bbb.bowler\n",
        "                           order by NoOfWickets desc;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (D)\n",
        "    def get_query4_result(self, con):\n",
        "        cur=con.execute(\"\"\"select batsman, (sum(batsman_runs)/count(ball)*100) as strikeRate from ipl_ball_by_ball\n",
        "                           where (extras_type not in (\"wides\",\"noballs\")) \n",
        "                           and (overs >= 7 and overs <= 20)\n",
        "                           group by batsman\n",
        "                           order by strikeRate desc;\"\"\")\n",
        "\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (E)\n",
        "    def get_query5_result(self, con):\n",
        "        cur=con.execute(\"\"\"select v.venue, v.city, sum(bbb.extra_runs) extra_runs from ipl_ball_by_ball bbb\n",
        "                           inner join ipl_matches as m on bbb.match_id=m.match_id\n",
        "                           inner join ipl_venue as v on m.venue_id =v.venue_id\n",
        "                           group by venue, v.city\n",
        "                           order by extra_runs desc;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (F)\n",
        "    def get_query6_result(self, con):\n",
        "        cur=con.execute(\"\"\"select m.player_of_match, count(m.player_of_match) from ipl_matches m\n",
        "                           join ipl_venue v on m.venue_id = v.venue_id\n",
        "                           where neutral_venue = 1\n",
        "                           group by m.player_of_match\n",
        "                           order by count(m.player_of_match) desc;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (H)\n",
        "    def get_query7_result(self, con):\n",
        "        cur = con.execute(\"\"\"select umpire, count(*) as umpire_count from\n",
        "                               (select umpire1 as umpire from ipl_matches union all\n",
        "                                 select umpire2 from ipl_matches) ipl_matches\n",
        "                                    group by ipl_matches.umpire\n",
        "                                    order by count(*) desc\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n",
        "    \n",
        "    # Question (I)\n",
        "    def get_query8_result(self, con):\n",
        "        cur= con.execute(\"\"\"select m.match_id, v.venue, v.city, sum(b.batsman_runs) TotalRuns from ipl_ball_by_ball as b\n",
        "                            inner join ipl_matches as m on b.match_id=m.match_id\n",
        "                            inner join ipl_venue as v on m.venue_id =v.venue_id\n",
        "                            where b.batsman= 'V Kohli'\n",
        "                            group by m.match_id, v.venue, v.city\n",
        "                            order by sum(b.batsman_runs) desc limit 1;\"\"\")\n",
        "        desc = cur.description\n",
        "        column_names = [col[0] for col in desc]\n",
        "        data = [dict(zip(column_names, row))  \n",
        "        for row in cur.fetchall()]\n",
        "        return data\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "65f06eaf-f7ab-4ac4-8637-5b1e74866cc8"
        },
        "id": "ukrkKa4RtkR5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# creating class database object\n",
        "db = Database()\n",
        " \n",
        "# calling the instance method using the object db\n",
        "con = db.get_conn()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "40a372e3-e3f0-44b7-9f4a-a18a116a4b1c"
        },
        "id": "Rul2Rm23tkR8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry1_result = db.get_query1_result(con);\n",
        "print(qry1_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b6a0e400-a927-44da-a836-d20d0aa61fb6"
        },
        "id": "xtknA2kStkR9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry2_result = db.get_query2_result(con);\n",
        "print(qry2_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "eca61e1d-b4ed-43f7-9f7d-68da6dab2132"
        },
        "id": "3Tb9sXsWtkR9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry3_result = db.get_query3_result(con);\n",
        "print(qry3_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4105e732-ce97-44d3-b9bd-a72e3278d004"
        },
        "id": "TZUGEKADtkR-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry4_result = db.get_query4_result(con);\n",
        "print(qry4_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "45da2141-fffd-4079-917a-410a51191184"
        },
        "id": "WOonuaIAtkR_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry5_result = db.get_query5_result(con);\n",
        "print(qry5_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "92b61f2e-fa10-412d-9fb4-8121f687a177"
        },
        "id": "PEKbn8YNtkSA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry6_result = db.get_query6_result(con);\n",
        "print(qry6_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "21a3549d-1f09-4d41-8323-21243e9c5f1e"
        },
        "id": "e0tOBeDNtkSA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry7_result = db.get_query7_result(con);\n",
        "print(qry7_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8e76ce70-f49f-4bf0-88e0-8e1db1ffc39f"
        },
        "id": "RLn9YSw_tkSB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qry8_result = db.get_query8_result(con);\n",
        "print(qry8_result)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4f7ad31d-4ae1-4dd5-82de-44708ce40a2c"
        },
        "id": "H4o9deQFtkSC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "db.close_conn(con)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "177ab62b-4379-4630-8b07-9f6bd09f267a"
        },
        "id": "E4ltAF0VtkSD"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "DataGrokr Assignment",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 3973652684757661
    },
    "colab": {
      "name": "IPL Data Analysis with PySpark",
      "provenance": [],
      "collapsed_sections": [
        "S5Hk2M5WtkRG"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}